# LLM Service

## Overview

This directory contains the implementation of the LLM (Large Language Model) service, which is responsible for handling interactions with large language models. The service is built using FastAPI and provides endpoints for processing text data, generating responses, and managing model configurations.

## Setup Instructions

1. Navigate to the `llm_service` directory:
   ```bash
   cd llm_service
   ```

2. Run the docker-compose command to start the service:
   ```bash
    docker-compose up
    ```

3. The LLM service will be accessible at `http://localhost:3002`.

4. To stop the service, use:
   ```bash
   docker-compose down
   ```

## Technologies Used

The llm service is built using the following technologies:

- **FastAPI** as the web framework;
- **Uvicorn** as the ASGI server;
- **Pydantic** for data validation;
- **Google Generative AI** for interacting with language models;
- **python-dotenv** for managing environment variables.

## Project Structure

The project structure is as follows:

```
llm_service/
├── app/
│   ├── routes/             # API route definitions
│   ├── schemas/            # Pydantic models for request and response validation
│   ├── services/           # Business logic and service implementations
│       ├── models/         # Available LLM models and configurations
│       ├── prompts/        # Predefined prompts for prompt engineering
│       ├── llm_engine.py   # Core logic for interacting with LLMs
│   ├── main.py             # Entry point for the FastAPI application
├── Dockerfile              # Dockerfile for building the service image
├── docker-compose.yml      # Docker Compose configuration file
├── requirements.txt        # Python dependencies
├── README.md               # This file
```

## Environment Variables

The LLM service uses environment variables for configuration. You can set these variables in a `.env` file in the `llm_service` directory if you intend to run the service solo. (if running as part of the full application stack, the main `.env` file in the root directory will be used).

The following environment variables are used:
- `GEMINI_API_KEY`: API key for accessing Google Generative AI services.

Here is an example `.env` file:

```
GEMINI_API_KEY=your_google_generative_ai_api_key
```

## API Endpoints

The LLM service provides the following API endpoints:

- `POST /answerprompt`: Accepts a prompt and returns a response generated by the specified LLM model.

### Request

- **Method**: POST
- **Request Body**:
    - `prompt` (string): The input prompt for the LLM.
    - `model` (string): The LLM model to use (e.g., "gemini").
    - `temperature` (float, optional): The temperature setting for response generation. The default is 0.7.
    - `chunking` (boolean, optional): Whether the model will use complete documents or chunks for in-context learning. The default is False.

### Response

- **Content**: A JSON object containing the answer and its sources.
```json
{
    "answer": "The generated answer to the prompt.",
    "sources": [
        {"document_name": "file1.txt", "quote": "Relevant context from file1."},
        {"document_name": "file2.txt", "quote": "Relevant context from file2."}
    ]
}
```

## Available Models

At the moment we just have one model available:
- `gemini-2.5-pro`: It is the one used by default when no model is specified, but you can also specify it explicitly by setting the `model` field in the request body to `"gemini"`.

It's important to mention taht although we have only one model available, the architecture is designed to easily accommodate additional models in the future.

## Important Notes

It's important to note the following:

- In order to generate the best possible answers, we use in-context learning with examples to show the model how we expect it to behave. For this we have a folder for each model, where we store the examples that we want to use for that specific model. They can either be human or machine generated examples, but the important thing to have in mind is that the better the examples, the better the answers. In addition you should consider balancing the number of examples with the size of the prompt, since it can reduce performance if the prompt is too large.